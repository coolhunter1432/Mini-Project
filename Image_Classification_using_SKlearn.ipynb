{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Classification using SKlearn.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPN5857gl4YQZL16tS5r6dD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coolhunter1432/Mini-Project/blob/main/Image_Classification_using_SKlearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49yy327L4iqI"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\r\n",
        "\r\n",
        "# Load data from https://www.openml.org/d/554\r\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZmw0Dgr4seY"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "## Look at a few random examples\r\n",
        "example_images = np.concatenate(\r\n",
        "    (np.concatenate(\r\n",
        "        [\r\n",
        "          X[np.random.randint(1, high=X.shape[0]-1), :].reshape(28, 28) for _ in range(5)\r\n",
        "        ],\r\n",
        "        axis=1\r\n",
        "    ),\r\n",
        "    np.concatenate(\r\n",
        "        [\r\n",
        "          X[np.random.randint(1, high=X.shape[0]-1), :].reshape(28, 28) for _ in range(5)\r\n",
        "        ],\r\n",
        "        axis=1\r\n",
        "    )), axis=0\r\n",
        "  )\r\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\r\n",
        "ax.imshow(example_images, cmap='gray')\r\n",
        "ax.axis('off')\r\n",
        "plt.tight_layout()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP696TMy4vE7"
      },
      "source": [
        "# preprocessing\r\n",
        "from sklearn.utils import check_random_state\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "random_state = check_random_state(0)\r\n",
        "permutation = random_state.permutation(X.shape[0])\r\n",
        "X = X[permutation]\r\n",
        "y = y[permutation]\r\n",
        "X = X.reshape((X.shape[0], -1))\r\n",
        "train_samples = 50000\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(\r\n",
        "    X, y, train_size=train_samples, test_size=10000)\r\n",
        "\r\n",
        "scaler = StandardScaler()\r\n",
        "MNIST_X_train = scaler.fit_transform(X_train)\r\n",
        "MNIST_X_test = scaler.transform(X_test)\r\n",
        "MNIST_y_train = y_train\r\n",
        "MNIST_y_test = y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpXsYxxf4x0d"
      },
      "source": [
        "! wget http://www.josiahwang.com/dataset/leedsbutterfly/leedsbutterfly_dataset_v1.0.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHXX-KvJ40mr"
      },
      "source": [
        "! unzip -q leedsbutterfly_dataset_v1.0.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQO43Ih243xE"
      },
      "source": [
        "! cat leedsbutterfly/README.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMMbAchF46xQ"
      },
      "source": [
        "# Load Classnames\r\n",
        "num_classes = 10\r\n",
        "classes = []\r\n",
        "for i in range(1, num_classes + 1):\r\n",
        "  with open('leedsbutterfly/descriptions/{0:03d}.txt'.format(i)) as d_file:\r\n",
        "    # the file has 3 lines. we'll read the firat line and ingore,\r\n",
        "    # and grab the second line, which is the common name for each class.\r\n",
        "    d_file.readline()\r\n",
        "    common_name = d_file.readline().strip()\r\n",
        "    classes.append(common_name)\r\n",
        "\r\n",
        "print(*classes, sep='\\n')\r\n",
        "butterfly_classes = classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkTAKi4k4-r3"
      },
      "source": [
        "# Load Images. We'll use imageio for image loading, as well as the os package \r\n",
        "# to traverse the directory.\r\n",
        "import os\r\n",
        "from imageio import imread as imread\r\n",
        "\r\n",
        "base_dir = 'leedsbutterfly/images'\r\n",
        "\r\n",
        "images_dict = {\r\n",
        "    '{0:03d}'.format(i): [] for i in range(1, num_classes + 1)\r\n",
        "}\r\n",
        "\r\n",
        "for image_filename in sorted(os.listdir('leedsbutterfly/images')):\r\n",
        "  id = image_filename.split('.')[0]\r\n",
        "  class_id = id[:3]\r\n",
        "  class_index = id[3:]\r\n",
        "  images_dict[class_id].append(os.path.join(base_dir, image_filename))\r\n",
        "\r\n",
        "dataset = {}\r\n",
        "\r\n",
        "# split 10% for validation.\r\n",
        "for class_id in images_dict:\r\n",
        "  images_list = images_dict[class_id]\r\n",
        "  train_stop = int(len(images_list) * 0.9)\r\n",
        "  train_files = images_list[:train_stop]\r\n",
        "  validation_files = images_list[train_stop:]\r\n",
        "  train_images = [imread(f) for f in train_files]\r\n",
        "  validation_images = [imread(f) for f in validation_files]\r\n",
        "  dataset[class_id] = {\r\n",
        "      'train': train_images,\r\n",
        "      'val': validation_images,\r\n",
        "      'train_f': train_files,   # keep filenames for printing purposes\r\n",
        "      'val_f': validation_files\r\n",
        "  }\r\n",
        "\r\n",
        "for class_id in dataset:\r\n",
        "  print('train {}'.format(class_id), dataset[class_id]['train_f'], sep='\\n')\r\n",
        "  print('val {}'.format(class_id), dataset[class_id]['val_f'], sep='\\n')\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh7-b0ij5Drm"
      },
      "source": [
        "from tqdm import tqdm\r\n",
        "import random\r\n",
        "import skimage.transform as transforms\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "\r\n",
        "class FeatureExtractor:\r\n",
        "  def __init__(self, in_features=15, out_classes=10):\r\n",
        "    self.pca = PCA(n_components = in_features)\r\n",
        "    self.out_classes = out_classes\r\n",
        "    self.training = True\r\n",
        "\r\n",
        "  def extract_features(self, x):\r\n",
        "    self.pca.fit((x[:, :, 0] + x[:,:,1] + x[:,:,2])/3)\r\n",
        "    p = self.pca.singular_values_\r\n",
        "    return p\r\n",
        "\r\n",
        "class SimpleDataloader:\r\n",
        "  def __init__(self, images_dict, shuffle=False, width=224, height=224):\r\n",
        "    self.feature_extractor = FeatureExtractor()\r\n",
        "    self.images_dict = images_dict\r\n",
        "    self.mode = 'train'\r\n",
        "    self.train_list = []\r\n",
        "    self.val_list = []\r\n",
        "    self.shuffle = shuffle\r\n",
        "    self.width = width\r\n",
        "    self.height = height\r\n",
        "    self.train_images = []\r\n",
        "    self.val_images = []\r\n",
        "    self.train_features = []\r\n",
        "    self.val_features = []\r\n",
        "    pbar = tqdm(total=832)\r\n",
        "    for class_id in self.images_dict:\r\n",
        "      for i in range(len(self.images_dict[class_id]['train'])):\r\n",
        "        self.train_list.append((class_id, i))\r\n",
        "        image = self.transform(self.images_dict[class_id]['train'][i])\r\n",
        "        features = self.feature_extractor.extract_features(image)\r\n",
        "        self.train_images.append(image)\r\n",
        "        self.train_features.append(features)\r\n",
        "        pbar.update(1)\r\n",
        "      for i in range(len(self.images_dict[class_id]['val'])):\r\n",
        "        self.val_list.append((class_id, i))\r\n",
        "        image = self.transform(self.images_dict[class_id]['val'][i])\r\n",
        "        features = self.feature_extractor.extract_features(image)\r\n",
        "        self.val_images.append(image)\r\n",
        "        self.val_features.append(features)\r\n",
        "        pbar.update(1)\r\n",
        "\r\n",
        "    self.set_mode('train', True)\r\n",
        "\r\n",
        "  def set_shuffle(self, shuffle):\r\n",
        "    self.shuffle = shuffle\r\n",
        "\r\n",
        "  def reset_shuffle(self):\r\n",
        "    self.indexes = list(range(len(self.data_list)))\r\n",
        "    if self.shuffle:\r\n",
        "      random.shuffle(self.indexes)\r\n",
        "      \r\n",
        "  def set_mode(self, mode, shuffle):\r\n",
        "    self.set_shuffle(shuffle)\r\n",
        "    assert mode == 'train' or mode == 'val', 'only supports training and validation'\r\n",
        "    self.mode = mode\r\n",
        "    if mode == 'train':\r\n",
        "      self.data_list = self.train_list\r\n",
        "      self.data = self.train_images\r\n",
        "      self.features = self.train_features\r\n",
        "    else:\r\n",
        "      self.data_list = self.val_list\r\n",
        "      self.data = self.val_images\r\n",
        "      self.features = self.val_features\r\n",
        "    self.reset_shuffle()\r\n",
        "\r\n",
        "  def transform(self, image):\r\n",
        "    # resize and center crop\r\n",
        "    # reshape smallest edge to match width and height\r\n",
        "    height, width, ch = image.shape\r\n",
        "    if width < height:\r\n",
        "      new_width = self.width\r\n",
        "      scale = self.width / width\r\n",
        "      new_height = int(height * scale)\r\n",
        "    else:\r\n",
        "      new_height = self.height\r\n",
        "      scale = self.height / height\r\n",
        "      new_width = int(width * scale)\r\n",
        "    image_tf = transforms.resize(image, (new_height, new_width))\r\n",
        "\r\n",
        "    # center crop\r\n",
        "    w_start = 0\r\n",
        "    w_stop = self.width\r\n",
        "    h_start = 0\r\n",
        "    h_stop = self.height\r\n",
        "    if new_width > self.width:\r\n",
        "      start = (new_width - self.width) // 2\r\n",
        "      w_start = start\r\n",
        "      w_stop = start + self.width\r\n",
        "    if new_height > self.height:\r\n",
        "      start = (new_height - self.height) // 2\r\n",
        "      h_start = start\r\n",
        "      h_stop = start + self.height\r\n",
        "    image_tf = image_tf[h_start:h_stop, w_start:w_stop, :]\r\n",
        "\r\n",
        "    return image_tf\r\n",
        "\r\n",
        "  def __getitem__(self, data_index):\r\n",
        "    i = self.indexes[data_index]\r\n",
        "    class_id, idx = self.data_list[i]\r\n",
        "    if data_index == len(self.data_list):\r\n",
        "      self.reset_shuffle\r\n",
        "    return self.data[i], self.features[i], int(class_id)\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.data_list)\r\n",
        "\r\n",
        "# Create butterfly dataset\r\n",
        "butterfly_dataloader = SimpleDataloader(dataset, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouQurOBA5xOb"
      },
      "source": [
        "# Example dataloader usage\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# 1. set mode and shuffle\r\n",
        "butterfly_dataloader.set_mode('train', True) \r\n",
        "#print(len(butterfly_dataloader)) # expect 745 train, 87 test for 832 total\r\n",
        "imgs = []\r\n",
        "count = 0\r\n",
        "for i, (image, features, class_id) in enumerate((butterfly_dataloader)):\r\n",
        "  #print(class_id) # We expect the first class id to be random when shuffle is true\r\n",
        "                  # Here is where we would run whatever training iteration we wanted.\r\n",
        "  imgs.append(image)\r\n",
        "  count += 1\r\n",
        "  if count >= 10:\r\n",
        "    break\r\n",
        "example_images = np.concatenate([np.concatenate(imgs[:5], axis=1), np.concatenate(imgs[5:], axis=1)], axis=0)\r\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\r\n",
        "ax.imshow(example_images)\r\n",
        "ax.axis('off')\r\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubfyoAB352Wg"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\r\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "def gather_features_leeds(all_classes, mode, shuffle):\r\n",
        "  # Load preprocessed features from dataloader\r\n",
        "  butterfly_dataloader.set_mode(mode, shuffle) \r\n",
        "  image_features = []\r\n",
        "  classes = []\r\n",
        "  for j, (image, features, class_id) in enumerate(butterfly_dataloader):\r\n",
        "    if class_id not in all_classes:\r\n",
        "      continue\r\n",
        "    classes.append(list(all_classes).index(class_id))\r\n",
        "    image_features.append(features)\r\n",
        "  scaler = StandardScaler()\r\n",
        "  classes = np.array(classes)\r\n",
        "  image_features = np.stack(image_features, axis=0)\r\n",
        "  return image_features, classes\r\n",
        "\r\n",
        "def gather_features_mnist(mode):\r\n",
        "  if mode == 'train':\r\n",
        "    return MNIST_X_train, MNIST_y_train\r\n",
        "  else:\r\n",
        "    return MNIST_X_test, MNIST_y_test\r\n",
        "\r\n",
        "def train(classifier, all_classes=None, data='MNIST', quiet=False, return_lda=False):\r\n",
        "  # load train data\r\n",
        "  if data == 'MNIST':\r\n",
        "    if not quiet:\r\n",
        "      print(\"TRAINING MNIST\")\r\n",
        "    train_features, train_classes = gather_features_mnist('train')\r\n",
        "  else:\r\n",
        "    if not quiet:\r\n",
        "      print(\"TRAINING LEEDS\")\r\n",
        "    train_features, train_classes = gather_features_leeds(all_classes, 'train', True)\r\n",
        "  # train LDA and train classifier\r\n",
        "  lda = LinearDiscriminantAnalysis(n_components=1).fit(train_features, train_classes)\r\n",
        "  train_features = lda.transform(train_features)\r\n",
        "  classifier.fit(train_features, train_classes)\r\n",
        "\r\n",
        "  # score train classification\r\n",
        "  train_acc = classifier.score(train_features, train_classes)\r\n",
        "\r\n",
        "  # load validation data\r\n",
        "  if data == 'MNIST':\r\n",
        "    val_features, val_classes = gather_features_mnist('test')\r\n",
        "  else:\r\n",
        "    val_features, val_classes = gather_features_leeds(all_classes, 'val', False)\r\n",
        "  # transform LDA\r\n",
        "  val_features = lda.transform(val_features)\r\n",
        "  # score validation classification\r\n",
        "  val_acc = classifier.score(val_features, val_classes)\r\n",
        "\r\n",
        "  if return_lda:\r\n",
        "    return train_acc, val_acc, lda\r\n",
        "  return train_acc, val_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8oMv5RM55cK"
      },
      "source": [
        "# select 2 random classes\r\n",
        "all_classes = np.random.choice(range(1, 11), 2, replace=False)\r\n",
        "features = 5\r\n",
        "depth = 18\r\n",
        "estimators = 15\r\n",
        "classifier = AdaBoostClassifier(\r\n",
        "    DecisionTreeClassifier(max_depth=depth),\r\n",
        "    n_estimators=estimators,\r\n",
        "    learning_rate=1.5,\r\n",
        "    algorithm=\"SAMME\"\r\n",
        ")\r\n",
        "t_acc, v_acc = train(classifier)\r\n",
        "print(\"train_acc {} val acc {}, random {}\".format(t_acc, v_acc, 1./10))\r\n",
        "classifier = AdaBoostClassifier(\r\n",
        "    DecisionTreeClassifier(max_depth=depth),\r\n",
        "    n_estimators=estimators,\r\n",
        "    learning_rate=1.5,\r\n",
        "    algorithm=\"SAMME\"\r\n",
        ")\r\n",
        "t_acc, v_acc = train(classifier, all_classes, 'LEEDS')\r\n",
        "print(\"train_acc {} val acc {}, random {}\".format(t_acc, v_acc, 1./2))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_986wlPi58D5"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\r\n",
        "all_classes = np.random.choice(range(1, 11), 2, replace=False)\r\n",
        "classifier = LogisticRegression(\r\n",
        "                C=1000. / train_samples, penalty='l2', solver='saga', tol=0.1\r\n",
        "             )\r\n",
        "t_acc, v_acc = train(classifier)\r\n",
        "print(\"train_acc {} val acc {}, random {}\".format(t_acc, v_acc, 1./10))\r\n",
        "\r\n",
        "classifier = LogisticRegression(\r\n",
        "                C=1000. / len(butterfly_dataloader), penalty='l2', solver='saga', tol=0.01\r\n",
        "             )\r\n",
        "t_acc, v_acc = train(classifier, all_classes, 'LEEDS')\r\n",
        "print(\"train_acc {} val acc {}, random {}\".format(t_acc, v_acc, 1./2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JWpU1yg5-a7"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\r\n",
        "all_classes = np.random.choice(range(1, 11), 2, replace=False)\r\n",
        "classifier = MLPClassifier(solver='lbfgs', alpha=1e-5,\r\n",
        "                    hidden_layer_sizes=(15, 15), random_state=1, max_iter=10000, warm_start=True)\r\n",
        "t_acc, v_acc = train(classifier)\r\n",
        "print(\"train_acc {} val acc {}, random {}\".format(t_acc, v_acc, 1./10))\r\n",
        "\r\n",
        "classifier = MLPClassifier(solver='lbfgs', alpha=1e-5,\r\n",
        "                    hidden_layer_sizes=(15, 15), random_state=1, max_iter=10000, warm_start=True)\r\n",
        "t_acc, v_acc = train(classifier, all_classes, 'LEEDS')\r\n",
        "print(\"train_acc {} val acc {}, random {}\".format(t_acc, v_acc, 1./2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E9cDi_s6Aym"
      },
      "source": [
        "ada_classifiers = []\r\n",
        "t_accs, v_accs = [], []\r\n",
        "random_acc = [1./N for N in range(2, 11)]\r\n",
        "for c in range(2, 11):\r\n",
        "  added = False\r\n",
        "  best = ()\r\n",
        "  best_v = 0\r\n",
        "  for depth in range(2, 18, 2):\r\n",
        "    for estimators in range(3, 15, 3):\r\n",
        "      if c == 10:\r\n",
        "        all_classes = np.array(range(1, 11))\r\n",
        "      else:\r\n",
        "        all_classes = np.random.choice(range(1, 11), c, replace=False)\r\n",
        "      classifier = AdaBoostClassifier(\r\n",
        "          DecisionTreeClassifier(max_depth=depth),\r\n",
        "          n_estimators=estimators,\r\n",
        "          learning_rate=1.5,\r\n",
        "          algorithm=\"SAMME\"\r\n",
        "      )\r\n",
        "      t_acc, v_acc, lda = train(classifier, all_classes, \"LEEDS\", True, True)\r\n",
        "      if v_acc > best_v:\r\n",
        "        best = (t_acc, v_acc)\r\n",
        "        if added:\r\n",
        "          ada_classifiers[-1] = (classifier, lda)\r\n",
        "        else:\r\n",
        "          ada_classifiers.append((classifier, lda))\r\n",
        "          added = True\r\n",
        "  t_accs.append(best[0])\r\n",
        "  v_accs.append(best[1])\r\n",
        "  print(\"{}: train acc {} val acc {} random {}\".format(c, best[0], best[1], 1./c))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BJHACW56D7j"
      },
      "source": [
        "import random\r\n",
        "mlp_classifiers = []\r\n",
        "t_accs_mlp, v_accs_mlp = [], []\r\n",
        "random_acc = [1./N for N in range(2, 11)]\r\n",
        "for c in range(2, 11):\r\n",
        "  added = False\r\n",
        "  for hidden in range(5, 20, 5):\r\n",
        "    best = ()\r\n",
        "    best_v = 0\r\n",
        "    if c == 10:\r\n",
        "      all_classes = np.array(range(1, 11))\r\n",
        "    else:\r\n",
        "      all_classes = np.random.choice(range(1, 11), c, replace=False)\r\n",
        "    features = 5\r\n",
        "    depth = 18\r\n",
        "    estimators = 15\r\n",
        "    classifier =  MLPClassifier(solver='lbfgs', alpha=1e-5,\r\n",
        "                hidden_layer_sizes=(hidden, hidden), random_state=1,\r\n",
        "                max_iter=10000, warm_start=True, early_stopping=True\r\n",
        "    )\r\n",
        "    t_acc, v_acc, lda = train(classifier, all_classes, 'LEEDS', True, True)\r\n",
        "    if v_acc > best_v:\r\n",
        "      best = (t_acc, v_acc)\r\n",
        "      if added:\r\n",
        "        mlp_classifiers[-1] = (classifier, lda)\r\n",
        "      else:\r\n",
        "        mlp_classifiers.append((classifier, lda))\r\n",
        "  t_accs_mlp.append(best[0])\r\n",
        "  v_accs_mlp.append(best[1])\r\n",
        "  print(\"{}: train acc {} val acc {} random {}\".format(c, best[0], best[1], 1./c))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpqUzRut6HjF"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 10))\r\n",
        "ax.plot(range(2, 11), random_acc, '#FF000080', label=\"random\")\r\n",
        "ax.plot(range(2, 11), t_accs, '#008000FF', linestyle='--', linewidth=3.0, label=\"train accuracy AdaBoost\")\r\n",
        "ax.plot(range(2, 11), v_accs, '#0000FFFF', linestyle='--', linewidth=3.0, label=\"validation accuracy AdaBoost\")\r\n",
        "ax.plot(range(2, 11), t_accs_mlp, '#008000FF', linestyle='-',linewidth=3.0, label=\"train accuracy MLP\")\r\n",
        "ax.plot(range(2, 11), v_accs_mlp, '#0000FFFF', linestyle='-', linewidth=3.0, label=\"validation accuracy MLP\")\r\n",
        "ax.set_title('Multi Layer Perceptron vs AdaBoost vs Logistic Regression')\r\n",
        "plt.legend()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDxbkDnK6J-i"
      },
      "source": [
        "butterfly_dataloader.set_mode('val', True)\r\n",
        "\r\n",
        "# in hindsight, this breakdown is excessive.\r\n",
        "examples = {\r\n",
        "    'ada': {\r\n",
        "        'success': {i: [] for i in range(1, 11)},\r\n",
        "        'failure': {i: [] for i in range(1, 11)},\r\n",
        "        'labels_success': {i: [] for i in range(1, 11)},\r\n",
        "        'labels_failure_pred': {i: [] for i in range(1, 11)},\r\n",
        "        'labels_failure_gt': {i: [] for i in range(1, 11)},\r\n",
        "        'complete': {j: [False for i in range(1, 11)] for j in ['success', 'failure']},\r\n",
        "     },\r\n",
        "     'mlp': {\r\n",
        "        'success': {i: [] for i in range(1, 11)},\r\n",
        "        'failure': {i: [] for i in range(1, 11)},\r\n",
        "        'labels_success': {i: [] for i in range(1, 11)},\r\n",
        "        'labels_failure_pred': {i: [] for i in range(1, 11)},\r\n",
        "        'labels_failure_gt': {i: [] for i in range(1, 11)},\r\n",
        "        'complete': {j: [False for i in range(1, 11)] for j in ['success', 'failure']},\r\n",
        "     }\r\n",
        "}\r\n",
        "\r\n",
        "for k in range(9):\r\n",
        "  if (np.array(examples['ada']['complete']['success']).all() and\r\n",
        "      np.array(examples['ada']['complete']['failure']).all() and\r\n",
        "      np.array(examples['mlp']['complete']['success']).all() and\r\n",
        "      np.array(examples['mlp']['complete']['failure']).all()):\r\n",
        "      break\r\n",
        "  ada_classifier, ada_lda = ada_classifiers[k]\r\n",
        "  mlp_classifier, mlp_lda = mlp_classifiers[k]\r\n",
        "  # plot some success and failure examples from each class\r\n",
        "  for i, (image, features, class_id) in enumerate(butterfly_dataloader):\r\n",
        "    features_ada = ada_lda.transform(features.reshape(1, 15))\r\n",
        "    features_mlp = mlp_lda.transform(features.reshape(1, 15))\r\n",
        "    pred_ada = int(ada_classifier.predict(features_ada.reshape(-1, 1)))\r\n",
        "    pred_mlp = int(mlp_classifier.predict(features_mlp.reshape(-1, 1)))\r\n",
        "    \r\n",
        "    success_ada = pred_ada == class_id\r\n",
        "    success_mlp = pred_mlp == class_id\r\n",
        "    if not success_ada and not examples['ada']['complete']['failure'][class_id-1]:\r\n",
        "        examples['ada']['failure'][class_id].append(image)\r\n",
        "        examples['ada']['labels_failure_pred'][class_id].append(butterfly_classes[pred_mlp-1])\r\n",
        "        examples['ada']['labels_failure_gt'][class_id].append(butterfly_classes[class_id-1])\r\n",
        "        if len(examples['ada']['failure'][class_id]) >= 1:\r\n",
        "          examples['ada']['complete']['failure'][class_id-1] = True\r\n",
        "      # ada failure\r\n",
        "    elif success_ada and not examples['ada']['complete']['success'][class_id-1]:\r\n",
        "        examples['ada']['success'][class_id].append(image)\r\n",
        "        examples['ada']['labels_success'][class_id].append(butterfly_classes[class_id-1])\r\n",
        "        if len(examples['ada']['success'][class_id]) >= 1:\r\n",
        "          examples['ada']['complete']['success'][class_id-1] = True\r\n",
        "      # ada success\r\n",
        "    elif not success_mlp  and not examples['mlp']['complete']['failure'][class_id-1]:\r\n",
        "        examples['mlp']['failure'][class_id].append(image)\r\n",
        "        examples['mlp']['labels_failure_pred'][class_id].append(butterfly_classes[pred_mlp-1])\r\n",
        "        examples['mlp']['labels_failure_gt'][class_id].append(butterfly_classes[class_id-1])\r\n",
        "        if len(examples['mlp']['failure'][class_id]) >= 1:\r\n",
        "          examples['mlp']['complete']['failure'][class_id-1] = True\r\n",
        "      # mlp failure\r\n",
        "    elif not examples['mlp']['complete']['success'][class_id-1]:\r\n",
        "        examples['mlp']['success'][class_id].append(image)\r\n",
        "        examples['mlp']['labels_success'][class_id].append(butterfly_classes[class_id-1])\r\n",
        "        if len(examples['mlp']['success'][class_id]) >= 1:\r\n",
        "          examples['mlp']['complete']['success'][class_id-1] = True\r\n",
        "      # mlp success"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOzekVkB6MTZ"
      },
      "source": [
        "count_success_mlp = 0\r\n",
        "count_failure_mlp = 0\r\n",
        "count_success_ada = 0\r\n",
        "count_failure_ada = 0\r\n",
        "for c in range(10):\r\n",
        "  try:\r\n",
        "    image_success_mlp = examples['mlp']['success'][c][0]\r\n",
        "    label_success_mlp = examples['mlp']['labels_success'][c][0]\r\n",
        "    fig, ax = plt.subplots(figsize=(3, 3))\r\n",
        "    ax.imshow(image_success_mlp)\r\n",
        "    ax.set_title('MLP Success:\\n {}'.format(label_success_mlp))\r\n",
        "    ax.axis('off')\r\n",
        "    ax.titlesize = 72\r\n",
        "    plt.tight_layout()\r\n",
        "    plt.savefig('success_mlp_{}.png'.format(count_success_mlp))\r\n",
        "    count_success_mlp += 1\r\n",
        "    plt.close()\r\n",
        "  except:\r\n",
        "    continue\r\n",
        "  try:\r\n",
        "    image_success_ada = examples['ada']['success'][c][0]\r\n",
        "    label_success_ada = examples['ada']['labels_success'][c][0]\r\n",
        "    fig, ax = plt.subplots(figsize=(3, 3))\r\n",
        "    ax.imshow(image_success_ada)\r\n",
        "    ax.set_title('AdaBoost Success:\\n {}'.format(label_success_mlp))\r\n",
        "    ax.axis('off')\r\n",
        "    ax.titlesize = 72\r\n",
        "    plt.tight_layout()\r\n",
        "    plt.savefig('success_ada_{}.png'.format(count_success_ada))\r\n",
        "    count_success_ada += 1\r\n",
        "    plt.close()\r\n",
        "  except:\r\n",
        "    pass\r\n",
        "  try:\r\n",
        "    image_failure_mlp = examples['mlp']['failure'][c][0]\r\n",
        "    label_failure_mlp_gt = examples['mlp']['labels_failure_gt'][c][0]\r\n",
        "    label_failure_mlp_pred = examples['mlp']['labels_failure_pred'][c][0]\r\n",
        "    fig, ax = plt.subplots(figsize=(3, 3))\r\n",
        "    ax.imshow(image_failure_mlp)\r\n",
        "    ax.set_title('MLP Failure\\nGuess: {}\\nTruth: {}'.format(label_failure_mlp_pred, label_failure_mlp_gt))\r\n",
        "    ax.titlesize = 52\r\n",
        "    ax.axis('off')\r\n",
        "    plt.tight_layout()\r\n",
        "    plt.savefig('failure_mlp_{}.png'.format(count_failure_mlp))\r\n",
        "    count_failure_mlp += 1\r\n",
        "    plt.close()\r\n",
        "  except:\r\n",
        "    pass\r\n",
        "  try:\r\n",
        "    image_failure_ada = examples['ada']['failure'][c][0]\r\n",
        "    label_failure_ada_gt = examples['ada']['labels_failure_gt'][c][0]\r\n",
        "    label_failure_ada_pred = examples['ada']['labels_failure_pred'][c][0]\r\n",
        "    fig, ax = plt.subplots(figsize=(3, 3))\r\n",
        "    ax.imshow(image_failure_ada)\r\n",
        "    ax.set_title('AdaBoost Failure\\nGuess: {}\\nTruth: {}'.format(label_failure_ada_pred, label_failure_ada_gt))\r\n",
        "    ax.axis('off')\r\n",
        "    ax.titlesize = 52\r\n",
        "    plt.tight_layout()\r\n",
        "    plt.savefig('failure_ada_{}.png'.format(count_failure_ada))\r\n",
        "    count_failure_ada += 1\r\n",
        "    plt.close()\r\n",
        "  except:\r\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eo4WHz4K6OjC"
      },
      "source": [
        "# success plot\r\n",
        "fig, ax = plt.subplots(nrows=1, ncols=6, figsize=(15, 3))\r\n",
        "rng = list(range(count_success_mlp))\r\n",
        "random.shuffle(rng)\r\n",
        "for i, idx in enumerate(rng[:3]):\r\n",
        "  ax[i].imshow(plt.imread('success_mlp_{}.png'.format(idx)))\r\n",
        "  ax[i].axis('off')\r\n",
        "rng = list(range(count_success_ada))\r\n",
        "random.shuffle(rng)\r\n",
        "for i, idx in enumerate(rng[:3]):\r\n",
        "  ax[i+3].imshow(plt.imread('success_ada_{}.png'.format(idx)))\r\n",
        "  ax[i+3].axis('off')\r\n",
        "plt.tight_layout()\r\n",
        "plt.suptitle('Success Examples')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "fig, ax = plt.subplots(nrows=1, ncols=6, figsize=(15, 3))\r\n",
        "rng = list(range(count_failure_mlp))\r\n",
        "random.shuffle(rng)\r\n",
        "for i, idx in enumerate(rng[:3]):\r\n",
        "  ax[i].imshow(plt.imread('failure_mlp_{}.png'.format(idx)))\r\n",
        "  ax[i].axis('off')\r\n",
        "rng = list(range(count_failure_ada))\r\n",
        "random.shuffle(rng)\r\n",
        "for i, idx in enumerate(rng[:3]):\r\n",
        "  ax[i+3].imshow(plt.imread('failure_ada_{}.png'.format(idx)))\r\n",
        "  ax[i+3].axis('off')\r\n",
        "plt.tight_layout()\r\n",
        "plt.suptitle('Failure Examples')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}